{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n",
      "Using GPU: NVIDIA GeForce RTX 4080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available. Check your setup.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from datasets import Dataset\n",
    "\n",
    "# Function to read IOB files and split into paragraphs\n",
    "def read_iob_file(file_path):\n",
    "    \"\"\"Read IOB file and convert to token-label samples\"\"\"\n",
    "    examples = []\n",
    "    words, labels = [], []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                token, label = line.split(\"\\t\")\n",
    "                words.append(token)\n",
    "                labels.append(label.strip())\n",
    "            else:\n",
    "                # End of a sample\n",
    "                if words and labels:\n",
    "                    examples.append({\"tokens\": words, \"labels\": labels})\n",
    "                    words, labels = [], []\n",
    "    # Add the last example if file doesn't end with a blank line\n",
    "    if words and labels:\n",
    "        examples.append({\"tokens\": words, \"labels\": labels})\n",
    "    return examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 998, Val samples: 124, Test samples: 126\n"
     ]
    }
   ],
   "source": [
    "# Load datasets using the updated function\n",
    "train_path = 'C:\\\\S24-25\\\\TxM\\\\dataset\\\\train.tsv'\n",
    "val_path = 'C:\\\\S24-25\\\\TxM\\\\dataset\\\\val_gold.tsv'\n",
    "test_path = 'C:\\\\S24-25\\\\TxM\\\\dataset\\\\test_gold.tsv'\n",
    "train_data = read_iob_file(train_path)\n",
    "val_data = read_iob_file(val_path)\n",
    "test_data = read_iob_file(test_path)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}, Val samples: {len(val_data)}, Test samples: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 998/998 [00:00<00:00, 2607.07 examples/s]\n",
      "Map: 100%|██████████| 126/126 [00:00<00:00, 2532.93 examples/s]\n",
      "Map: 100%|██████████| 124/124 [00:00<00:00, 2656.52 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForPreTraining\n",
    "model_checkpoint=\"kamalkraj/bioelectra-base-discriminator-pubmed\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kamalkraj/bioelectra-base-discriminator-pubmed\")\n",
    "model = AutoModelForPreTraining.from_pretrained(\"kamalkraj/bioelectra-base-discriminator-pubmed\")\n",
    "# Define label mappings\n",
    "label_list = [\"O\", \"B-ADR\", \"I-ADR\", \"B-DRU\", \"I-DRU\", \"B-DIS\", \"I-DIS\", \"B-SYM\", \"I-SYM\"]\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",      # or True, depending on your preference\n",
    "        truncation=True,          # enable truncation\n",
    "        max_length=512,           # explicitly set maximum sequence length \n",
    "        return_overflowing_tokens=False,\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                aligned_labels.append(-100)  # Ignore padding\n",
    "            elif word_idx != previous_word_idx:\n",
    "                aligned_labels.append(label_to_id[label[word_idx]])\n",
    "            else:\n",
    "                aligned_labels.append(label_to_id[label[word_idx]])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = Dataset.from_list(train_data).map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset = Dataset.from_list(test_data).map(tokenize_and_align_labels, batched=True)\n",
    "val_dataset = Dataset.from_list(val_data).map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at kamalkraj/bioelectra-base-discriminator-pubmed and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train and evaluate with default parameters on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./bio_electra_logs\",\n",
    "    logging_steps=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/189 [02:27<43:57, 14.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6947, 'grad_norm': 8.60431957244873, 'learning_rate': 1.8941798941798943e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 20/189 [04:54<40:57, 14.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8684, 'grad_norm': 1.3569080829620361, 'learning_rate': 1.7883597883597884e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 30/189 [07:18<38:08, 14.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5876, 'grad_norm': 0.6973162293434143, 'learning_rate': 1.6825396825396828e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 40/189 [09:51<37:37, 15.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4674, 'grad_norm': 0.6656844615936279, 'learning_rate': 1.576719576719577e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 50/189 [12:23<35:14, 15.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4717, 'grad_norm': 0.5613645315170288, 'learning_rate': 1.470899470899471e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 60/189 [14:55<32:45, 15.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4353, 'grad_norm': 0.5676628351211548, 'learning_rate': 1.3650793650793652e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 63/189 [15:29<24:39, 11.75s/it]c:\\Users\\HikaLipp\\miniconda3\\envs\\TxM\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                \n",
      " 33%|███▎      | 63/189 [15:33<24:39, 11.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.39326393604278564, 'eval_ADR': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 584}, 'eval_DIS': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 39}, 'eval_DRU': {'precision': 0.8333333333333334, 'recall': 0.0176678445229682, 'f1': 0.03460207612456747, 'number': 283}, 'eval_SYM': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 44}, 'eval_overall_precision': 0.8333333333333334, 'eval_overall_recall': 0.005263157894736842, 'eval_overall_f1': 0.010460251046025104, 'eval_overall_accuracy': 0.8785520739248246, 'eval_runtime': 4.2187, 'eval_samples_per_second': 29.867, 'eval_steps_per_second': 1.896, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 70/189 [17:20<29:48, 15.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3473, 'grad_norm': 0.5149990916252136, 'learning_rate': 1.2592592592592593e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 80/189 [19:52<27:33, 15.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3737, 'grad_norm': 0.7052136063575745, 'learning_rate': 1.1534391534391536e-05, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 90/189 [22:24<25:02, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3659, 'grad_norm': 1.7506002187728882, 'learning_rate': 1.0476190476190477e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 100/189 [24:56<22:31, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3326, 'grad_norm': 0.5470075607299805, 'learning_rate': 9.417989417989418e-06, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 110/189 [27:28<20:05, 15.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.349, 'grad_norm': 1.2141553163528442, 'learning_rate': 8.35978835978836e-06, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 120/189 [30:00<17:26, 15.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3035, 'grad_norm': 0.6778350472450256, 'learning_rate': 7.301587301587301e-06, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 126/189 [31:19<12:16, 11.69s/it]c:\\Users\\HikaLipp\\miniconda3\\envs\\TxM\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                 \n",
      " 67%|██████▋   | 126/189 [31:23<12:16, 11.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3069069981575012, 'eval_ADR': {'precision': 0.2767203513909224, 'recall': 0.3236301369863014, 'f1': 0.2983425414364641, 'number': 584}, 'eval_DIS': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 39}, 'eval_DRU': {'precision': 0.7403314917127072, 'recall': 0.4734982332155477, 'f1': 0.5775862068965518, 'number': 283}, 'eval_SYM': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 44}, 'eval_overall_precision': 0.3738425925925926, 'eval_overall_recall': 0.34, 'eval_overall_f1': 0.35611907386990077, 'eval_overall_accuracy': 0.9043979712360175, 'eval_runtime': 4.3421, 'eval_samples_per_second': 29.018, 'eval_steps_per_second': 1.842, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 130/189 [32:26<14:45, 15.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2995, 'grad_norm': 1.240178108215332, 'learning_rate': 6.243386243386243e-06, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 140/189 [34:58<12:22, 15.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2831, 'grad_norm': 1.5248281955718994, 'learning_rate': 5.185185185185185e-06, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 150/189 [37:31<09:57, 15.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.276, 'grad_norm': 0.6992630958557129, 'learning_rate': 4.126984126984127e-06, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 160/189 [40:04<07:17, 15.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3054, 'grad_norm': 1.071355938911438, 'learning_rate': 3.068783068783069e-06, 'epoch': 2.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 170/189 [42:38<04:49, 15.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2619, 'grad_norm': 0.8284920454025269, 'learning_rate': 2.0105820105820108e-06, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 180/189 [45:11<02:16, 15.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2951, 'grad_norm': 0.7920874357223511, 'learning_rate': 9.523809523809525e-07, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [47:15<00:00, 11.67s/it]c:\\Users\\HikaLipp\\miniconda3\\envs\\TxM\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                 \n",
      "100%|██████████| 189/189 [47:21<00:00, 11.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28889429569244385, 'eval_ADR': {'precision': 0.2753623188405797, 'recall': 0.3578767123287671, 'f1': 0.31124348473566643, 'number': 584}, 'eval_DIS': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 39}, 'eval_DRU': {'precision': 0.7798165137614679, 'recall': 0.6007067137809188, 'f1': 0.6786427145708583, 'number': 283}, 'eval_SYM': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 44}, 'eval_overall_precision': 0.38792221084953943, 'eval_overall_recall': 0.3989473684210526, 'eval_overall_f1': 0.39335755059678257, 'eval_overall_accuracy': 0.9083582296949906, 'eval_runtime': 4.111, 'eval_samples_per_second': 30.649, 'eval_steps_per_second': 1.946, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [47:23<00:00, 15.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2843.3146, 'train_samples_per_second': 1.053, 'train_steps_per_second': 0.066, 'train_loss': 0.4534431260729593, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=0.4534431260729593, metrics={'train_runtime': 2843.3146, 'train_samples_per_second': 1.053, 'train_steps_per_second': 0.066, 'total_flos': 782372000176128.0, 'train_loss': 0.4534431260729593, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from evaluate import load\n",
    "import torch\n",
    "\n",
    "# Load metric for evaluation\n",
    "metric = load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(predictions):\n",
    "    predictions, labels = predictions\n",
    "    predictions = torch.argmax(torch.tensor(predictions), dim=2)\n",
    "    \n",
    "    # Convert predictions and labels to CPU and then to plain Python integers\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    \n",
    "    true_labels = [\n",
    "        [id_to_label[label] for label in label_seq if label != -100] \n",
    "        for label_seq in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [id_to_label[int(p)] for (p, l) in zip(prediction, label_seq) if l != -100]  # Convert tensor to int\n",
    "        for prediction, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "    return metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:03<00:00,  1.62it/s]c:\\Users\\HikaLipp\\miniconda3\\envs\\TxM\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 8/8 [00:03<00:00,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.28889429569244385, 'eval_ADR': {'precision': 0.2753623188405797, 'recall': 0.3578767123287671, 'f1': 0.31124348473566643, 'number': 584}, 'eval_DIS': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 39}, 'eval_DRU': {'precision': 0.7798165137614679, 'recall': 0.6007067137809188, 'f1': 0.6786427145708583, 'number': 283}, 'eval_SYM': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 44}, 'eval_overall_precision': 0.38792221084953943, 'eval_overall_recall': 0.3989473684210526, 'eval_overall_f1': 0.39335755059678257, 'eval_overall_accuracy': 0.9083582296949906, 'eval_runtime': 4.4505, 'eval_samples_per_second': 28.311, 'eval_steps_per_second': 1.798, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train and test on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. HPO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TxM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
