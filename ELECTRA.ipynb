{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n",
      "Using GPU: NVIDIA GeForce RTX 4080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available. Check your setup.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HikaLipp\\miniconda3\\envs\\TxM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from datasets import Dataset\n",
    "\n",
    "# Function to read IOB files and split into paragraphs\n",
    "def read_iob_file(file_path):\n",
    "    \"\"\"Read IOB file and convert to token-label samples\"\"\"\n",
    "    examples = []\n",
    "    words, labels = [], []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                token, label = line.split(\"\\t\")\n",
    "                words.append(token)\n",
    "                labels.append(label.strip())\n",
    "            else:\n",
    "                # End of a sample\n",
    "                if words and labels:\n",
    "                    examples.append({\"tokens\": words, \"labels\": labels})\n",
    "                    words, labels = [], []\n",
    "    # Add the last example if file doesn't end with a blank line\n",
    "    if words and labels:\n",
    "        examples.append({\"tokens\": words, \"labels\": labels})\n",
    "    return examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 998, Val samples: 124, Test samples: 126\n"
     ]
    }
   ],
   "source": [
    "# Load datasets using the updated function\n",
    "train_path = 'C:\\\\S24-25\\\\TxM\\\\dataset\\\\train.tsv'\n",
    "val_path = 'C:\\\\S24-25\\\\TxM\\\\dataset\\\\val_gold.tsv'\n",
    "test_path = 'C:\\\\S24-25\\\\TxM\\\\dataset\\\\test_gold.tsv'\n",
    "train_data = read_iob_file(train_path)\n",
    "val_data = read_iob_file(val_path)\n",
    "test_data = read_iob_file(test_path)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}, Val samples: {len(val_data)}, Test samples: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HikaLipp\\miniconda3\\envs\\TxM\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HikaLipp\\.cache\\huggingface\\hub\\models--google--electra-small-discriminator. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of ElectraForMaskedLM were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['generator_lm_head.bias', 'generator_predictions.LayerNorm.bias', 'generator_predictions.LayerNorm.weight', 'generator_predictions.dense.bias', 'generator_predictions.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Map: 100%|██████████| 998/998 [00:00<00:00, 2574.11 examples/s]\n",
      "Map: 100%|██████████| 126/126 [00:00<00:00, 2393.78 examples/s]\n",
      "Map: 100%|██████████| 124/124 [00:00<00:00, 2523.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# Load model directly\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "model_checkpoint=\"google/electra-small-discriminator\"\n",
    "pipe = pipeline(\"fill-mask\", model=\"google/electra-small-discriminator\")\n",
    "# Define label mappings\n",
    "label_list = [\"O\", \"B-ADR\", \"I-ADR\", \"B-DRU\", \"I-DRU\", \"B-DIS\", \"I-DIS\", \"B-SYM\", \"I-SYM\"]\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = pipe.tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",      # or True, depending on your preference\n",
    "        truncation=True,          # enable truncation\n",
    "        max_length=512,           # explicitly set maximum sequence length \n",
    "        return_overflowing_tokens=False,\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                aligned_labels.append(-100)  # Ignore padding\n",
    "            elif word_idx != previous_word_idx:\n",
    "                aligned_labels.append(label_to_id[label[word_idx]])\n",
    "            else:\n",
    "                aligned_labels.append(label_to_id[label[word_idx]])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = Dataset.from_list(train_data).map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset = Dataset.from_list(test_data).map(tokenize_and_align_labels, batched=True)\n",
    "val_dataset = Dataset.from_list(val_data).map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train and evaluate with default parameters on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./electra_logs\",\n",
    "    logging_steps=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/189 [00:01<00:22,  8.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4572, 'grad_norm': 1.2515650987625122, 'learning_rate': 1.8941798941798943e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 21/189 [00:02<00:20,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4637, 'grad_norm': 0.9536014199256897, 'learning_rate': 1.7883597883597884e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 31/189 [00:03<00:19,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4724, 'grad_norm': 1.3160325288772583, 'learning_rate': 1.6825396825396828e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 41/189 [00:05<00:18,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4014, 'grad_norm': 1.4200518131256104, 'learning_rate': 1.576719576719577e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 51/189 [00:06<00:16,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.444, 'grad_norm': 0.8572367429733276, 'learning_rate': 1.470899470899471e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 61/189 [00:07<00:15,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.413, 'grad_norm': 0.8080768585205078, 'learning_rate': 1.3650793650793652e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 33%|███▎      | 63/189 [00:08<00:15,  8.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3888791501522064, 'eval_ADR': {'precision': 0.22287968441814596, 'recall': 0.1978984238178634, 'f1': 0.20964749536178107, 'number': 571}, 'eval_DIS': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 52}, 'eval_DRU': {'precision': 0.8444444444444444, 'recall': 0.25249169435215946, 'f1': 0.38874680306905374, 'number': 301}, 'eval_SYM': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 48}, 'eval_overall_precision': 0.3165829145728643, 'eval_overall_recall': 0.19444444444444445, 'eval_overall_f1': 0.2409177820267686, 'eval_overall_accuracy': 0.8924641314946371, 'eval_runtime': 0.5328, 'eval_samples_per_second': 236.504, 'eval_steps_per_second': 15.016, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 71/189 [00:09<00:16,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3581, 'grad_norm': 0.7626343369483948, 'learning_rate': 1.2592592592592593e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 81/189 [00:10<00:13,  8.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3847, 'grad_norm': 0.8594221472740173, 'learning_rate': 1.1534391534391536e-05, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 91/189 [00:11<00:11,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3953, 'grad_norm': 1.1106054782867432, 'learning_rate': 1.0476190476190477e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 101/189 [00:13<00:10,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3636, 'grad_norm': 0.9679637551307678, 'learning_rate': 9.417989417989418e-06, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 111/189 [00:14<00:09,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3854, 'grad_norm': 0.824306309223175, 'learning_rate': 8.35978835978836e-06, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 121/189 [00:15<00:08,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3473, 'grad_norm': 0.9817842841148376, 'learning_rate': 7.301587301587301e-06, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 125/189 [00:16<00:07,  8.34it/s]c:\\Users\\HikaLipp\\miniconda3\\envs\\TxM\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                 \n",
      " 67%|██████▋   | 126/189 [00:16<00:07,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34956198930740356, 'eval_ADR': {'precision': 0.23589001447178004, 'recall': 0.28546409807355516, 'f1': 0.2583201267828843, 'number': 571}, 'eval_DIS': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 52}, 'eval_DRU': {'precision': 0.8421052631578947, 'recall': 0.53156146179402, 'f1': 0.6517311608961304, 'number': 301}, 'eval_SYM': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 48}, 'eval_overall_precision': 0.36662883087400683, 'eval_overall_recall': 0.3323045267489712, 'eval_overall_f1': 0.3486238532110092, 'eval_overall_accuracy': 0.9028416213957375, 'eval_runtime': 0.5254, 'eval_samples_per_second': 239.829, 'eval_steps_per_second': 15.227, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 131/189 [00:17<00:09,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3597, 'grad_norm': 1.224724292755127, 'learning_rate': 6.243386243386243e-06, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 141/189 [00:18<00:05,  8.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3419, 'grad_norm': 1.2506340742111206, 'learning_rate': 5.185185185185185e-06, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 151/189 [00:19<00:04,  8.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3245, 'grad_norm': 1.064380407333374, 'learning_rate': 4.126984126984127e-06, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 161/189 [00:21<00:03,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3592, 'grad_norm': 1.0506058931350708, 'learning_rate': 3.068783068783069e-06, 'epoch': 2.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 171/189 [00:22<00:02,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3248, 'grad_norm': 1.1256532669067383, 'learning_rate': 2.0105820105820108e-06, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 181/189 [00:23<00:00,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3566, 'grad_norm': 1.0210720300674438, 'learning_rate': 9.523809523809525e-07, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 188/189 [00:24<00:00,  8.37it/s]c:\\Users\\HikaLipp\\miniconda3\\envs\\TxM\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                 \n",
      "100%|██████████| 189/189 [00:25<00:00,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3368615508079529, 'eval_ADR': {'precision': 0.2594142259414226, 'recall': 0.3257443082311734, 'f1': 0.2888198757763975, 'number': 571}, 'eval_DIS': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 52}, 'eval_DRU': {'precision': 0.8090909090909091, 'recall': 0.5913621262458472, 'f1': 0.6833013435700576, 'number': 301}, 'eval_SYM': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 48}, 'eval_overall_precision': 0.38847385272145146, 'eval_overall_recall': 0.37448559670781895, 'eval_overall_f1': 0.3813514929282347, 'eval_overall_accuracy': 0.9056971723081209, 'eval_runtime': 0.5631, 'eval_samples_per_second': 223.778, 'eval_steps_per_second': 14.208, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [00:25<00:00,  7.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 25.3804, 'train_samples_per_second': 117.965, 'train_steps_per_second': 7.447, 'train_loss': 0.38417816162109375, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=0.38417816162109375, metrics={'train_runtime': 25.3804, 'train_samples_per_second': 117.965, 'train_steps_per_second': 7.447, 'total_flos': 87493824681984.0, 'train_loss': 0.38417816162109375, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from evaluate import load\n",
    "import torch\n",
    "\n",
    "# Load metric for evaluation\n",
    "metric = load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(predictions):\n",
    "    predictions, labels = predictions\n",
    "    predictions = torch.argmax(torch.tensor(predictions), dim=2)\n",
    "    \n",
    "    # Convert predictions and labels to CPU and then to plain Python integers\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    \n",
    "    true_labels = [\n",
    "        [id_to_label[label] for label in label_seq if label != -100] \n",
    "        for label_seq in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [id_to_label[int(p)] for (p, l) in zip(prediction, label_seq) if l != -100]  # Convert tensor to int\n",
    "        for prediction, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "    return metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=pipe.tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [00:00<00:00, 18.28it/s]c:\\Users\\HikaLipp\\miniconda3\\envs\\TxM\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 8/8 [00:00<00:00, 14.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.3368615508079529, 'eval_ADR': {'precision': 0.2594142259414226, 'recall': 0.3257443082311734, 'f1': 0.2888198757763975, 'number': 571}, 'eval_DIS': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 52}, 'eval_DRU': {'precision': 0.8090909090909091, 'recall': 0.5913621262458472, 'f1': 0.6833013435700576, 'number': 301}, 'eval_SYM': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 48}, 'eval_overall_precision': 0.38847385272145146, 'eval_overall_recall': 0.37448559670781895, 'eval_overall_f1': 0.3813514929282347, 'eval_overall_accuracy': 0.9056971723081209, 'eval_runtime': 0.7153, 'eval_samples_per_second': 176.139, 'eval_steps_per_second': 11.183, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train and test on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. HPO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TxM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
