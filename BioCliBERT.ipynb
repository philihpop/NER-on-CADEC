{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n",
      "Using GPU: NVIDIA GeForce RTX 4080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available. Check your setup.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from datasets import Dataset\n",
    "\n",
    "# Function to read IOB files and split into paragraphs\n",
    "def read_iob_file(file_path):\n",
    "    \"\"\"Read IOB file and convert to token-label samples\"\"\"\n",
    "    examples = []\n",
    "    words, labels = [], []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                token, label = line.split(\"\\t\")\n",
    "                words.append(token)\n",
    "                labels.append(label.strip())\n",
    "            else:\n",
    "                # End of a sample\n",
    "                if words and labels:\n",
    "                    examples.append({\"tokens\": words, \"labels\": labels})\n",
    "                    words, labels = [], []\n",
    "    # Add the last example if file doesn't end with a blank line\n",
    "    if words and labels:\n",
    "        examples.append({\"tokens\": words, \"labels\": labels})\n",
    "    return examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 998, Val samples: 124, Test samples: 126\n"
     ]
    }
   ],
   "source": [
    "# Load datasets using the updated function\n",
    "train_path = 'C:\\\\S24-25\\\\TxM\\\\dataset\\\\train.tsv'\n",
    "val_path = 'C:\\\\S24-25\\\\TxM\\\\dataset\\\\val_gold.tsv'\n",
    "test_path = 'C:\\\\S24-25\\\\TxM\\\\dataset\\\\test_gold.tsv'\n",
    "train_data = read_iob_file(train_path)\n",
    "val_data = read_iob_file(val_path)\n",
    "test_data = read_iob_file(test_path)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}, Val samples: {len(val_data)}, Test samples: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 998/998 [00:00<00:00, 2624.21 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 126/126 [00:00<00:00, 2342.01 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 124/124 [00:00<00:00, 2486.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# Load model directly\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "model_checkpoint=\"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "pipe = pipeline(\"fill-mask\", model=\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "# Define label mappings\n",
    "label_list = [\"O\", \"B-ADR\", \"I-ADR\", \"B-DRU\", \"I-DRU\", \"B-DIS\", \"I-DIS\", \"B-SYM\", \"I-SYM\"]\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = pipe.tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",      # or True, depending on your preference\n",
    "        truncation=True,          # enable truncation\n",
    "        max_length=512,           # explicitly set maximum sequence length \n",
    "        return_overflowing_tokens=False,\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                aligned_labels.append(-100)  # Ignore padding\n",
    "            elif word_idx != previous_word_idx:\n",
    "                aligned_labels.append(label_to_id[label[word_idx]])\n",
    "            else:\n",
    "                aligned_labels.append(label_to_id[label[word_idx]])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = Dataset.from_list(train_data).map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset = Dataset.from_list(test_data).map(tokenize_and_align_labels, batched=True)\n",
    "val_dataset = Dataset.from_list(val_data).map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train and evaluate with default parameters on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./bio_bert_logs\",\n",
    "    logging_steps=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/189 [00:08<01:20,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.217, 'grad_norm': 1.288893461227417, 'learning_rate': 1.8941798941798943e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 20/189 [00:11<01:03,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2185, 'grad_norm': 1.2395459413528442, 'learning_rate': 1.7883597883597884e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 30/189 [00:15<00:58,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2213, 'grad_norm': 1.9911036491394043, 'learning_rate': 1.6825396825396828e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 40/189 [00:19<00:55,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1705, 'grad_norm': 0.9661669135093689, 'learning_rate': 1.576719576719577e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 50/189 [00:22<00:51,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2053, 'grad_norm': 1.5040099620819092, 'learning_rate': 1.470899470899471e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 60/189 [00:26<00:47,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1795, 'grad_norm': 1.3299952745437622, 'learning_rate': 1.3650793650793652e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 63/189 [00:27<00:38,  3.30it/s]c:\\Users\\HikaLipp\\miniconda3\\envs\\TxM\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 33%|███▎      | 63/189 [00:28<00:38,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23622222244739532, 'eval_ADR': {'precision': 0.43388429752066116, 'recall': 0.5189456342668863, 'f1': 0.47261815453863465, 'number': 607}, 'eval_DIS': {'precision': 0.5, 'recall': 0.017241379310344827, 'f1': 0.03333333333333333, 'number': 58}, 'eval_DRU': {'precision': 0.8398791540785498, 'recall': 0.8633540372670807, 'f1': 0.8514548238897398, 'number': 322}, 'eval_SYM': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 50}, 'eval_overall_precision': 0.5609065155807366, 'eval_overall_recall': 0.5728061716489875, 'eval_overall_f1': 0.566793893129771, 'eval_overall_accuracy': 0.9283537617661001, 'eval_runtime': 1.0603, 'eval_samples_per_second': 118.837, 'eval_steps_per_second': 7.545, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 70/189 [00:32<00:54,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1403, 'grad_norm': 0.9963726997375488, 'learning_rate': 1.2592592592592593e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 80/189 [00:36<00:40,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1632, 'grad_norm': 1.7293460369110107, 'learning_rate': 1.1534391534391536e-05, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 90/189 [00:39<00:36,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1715, 'grad_norm': 1.4716163873672485, 'learning_rate': 1.0476190476190477e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 100/189 [00:43<00:33,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1546, 'grad_norm': 0.7301039099693298, 'learning_rate': 9.417989417989418e-06, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 110/189 [00:47<00:29,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.162, 'grad_norm': 0.7526003122329712, 'learning_rate': 8.35978835978836e-06, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 120/189 [00:51<00:25,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1242, 'grad_norm': 0.7895763516426086, 'learning_rate': 7.301587301587301e-06, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 126/189 [00:53<00:19,  3.29it/s]c:\\Users\\HikaLipp\\miniconda3\\envs\\TxM\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 67%|██████▋   | 126/189 [00:54<00:19,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2372179478406906, 'eval_ADR': {'precision': 0.45874125874125876, 'recall': 0.5403624382207578, 'f1': 0.4962178517397882, 'number': 607}, 'eval_DIS': {'precision': 0.08333333333333333, 'recall': 0.017241379310344827, 'f1': 0.028571428571428574, 'number': 58}, 'eval_DRU': {'precision': 0.8377581120943953, 'recall': 0.8819875776397516, 'f1': 0.8593040847201211, 'number': 322}, 'eval_SYM': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 50}, 'eval_overall_precision': 0.575046904315197, 'eval_overall_recall': 0.5911282545805208, 'eval_overall_f1': 0.5829766999524488, 'eval_overall_accuracy': 0.9293695401909663, 'eval_runtime': 1.0783, 'eval_samples_per_second': 116.85, 'eval_steps_per_second': 7.419, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 130/189 [00:56<00:35,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1351, 'grad_norm': 1.334479570388794, 'learning_rate': 6.243386243386243e-06, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 140/189 [01:00<00:18,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1314, 'grad_norm': 1.2395824193954468, 'learning_rate': 5.185185185185185e-06, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 150/189 [01:04<00:14,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1358, 'grad_norm': 1.2166728973388672, 'learning_rate': 4.126984126984127e-06, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 160/189 [01:08<00:10,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1453, 'grad_norm': 1.1995375156402588, 'learning_rate': 3.068783068783069e-06, 'epoch': 2.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 170/189 [01:11<00:07,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1272, 'grad_norm': 1.2140284776687622, 'learning_rate': 2.0105820105820108e-06, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 180/189 [01:15<00:03,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.158, 'grad_norm': 1.1432398557662964, 'learning_rate': 9.523809523809525e-07, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [01:18<00:00,  3.28it/s]c:\\Users\\HikaLipp\\miniconda3\\envs\\TxM\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|██████████| 189/189 [01:20<00:00,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22910471260547638, 'eval_ADR': {'precision': 0.4652482269503546, 'recall': 0.5403624382207578, 'f1': 0.5, 'number': 607}, 'eval_DIS': {'precision': 0.21739130434782608, 'recall': 0.08620689655172414, 'f1': 0.12345679012345678, 'number': 58}, 'eval_DRU': {'precision': 0.8636363636363636, 'recall': 0.8850931677018633, 'f1': 0.8742331288343559, 'number': 322}, 'eval_SYM': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 50}, 'eval_overall_precision': 0.5841209829867675, 'eval_overall_recall': 0.5959498553519769, 'eval_overall_f1': 0.5899761336515513, 'eval_overall_accuracy': 0.9297758515609128, 'eval_runtime': 1.0596, 'eval_samples_per_second': 118.912, 'eval_steps_per_second': 7.55, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [01:22<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 82.0785, 'train_samples_per_second': 36.477, 'train_steps_per_second': 2.303, 'train_loss': 0.16333529431983906, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=0.16333529431983906, metrics={'train_runtime': 82.0785, 'train_samples_per_second': 36.477, 'train_steps_per_second': 2.303, 'total_flos': 782372000176128.0, 'train_loss': 0.16333529431983906, 'epoch': 3.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from evaluate import load\n",
    "import torch\n",
    "\n",
    "# Load metric for evaluation\n",
    "metric = load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(predictions):\n",
    "    predictions, labels = predictions\n",
    "    predictions = torch.argmax(torch.tensor(predictions), dim=2)\n",
    "    \n",
    "    # Convert predictions and labels to CPU and then to plain Python integers\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    \n",
    "    true_labels = [\n",
    "        [id_to_label[label] for label in label_seq if label != -100] \n",
    "        for label_seq in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [id_to_label[int(p)] for (p, l) in zip(prediction, label_seq) if l != -100]  # Convert tensor to int\n",
    "        for prediction, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "    return metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=pipe.tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [00:00<00:00,  9.28it/s]c:\\Users\\HikaLipp\\miniconda3\\envs\\TxM\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 8/8 [00:00<00:00,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.22910471260547638, 'eval_ADR': {'precision': 0.4652482269503546, 'recall': 0.5403624382207578, 'f1': 0.5, 'number': 607}, 'eval_DIS': {'precision': 0.21739130434782608, 'recall': 0.08620689655172414, 'f1': 0.12345679012345678, 'number': 58}, 'eval_DRU': {'precision': 0.8636363636363636, 'recall': 0.8850931677018633, 'f1': 0.8742331288343559, 'number': 322}, 'eval_SYM': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 50}, 'eval_overall_precision': 0.5841209829867675, 'eval_overall_recall': 0.5959498553519769, 'eval_overall_f1': 0.5899761336515513, 'eval_overall_accuracy': 0.9297758515609128, 'eval_runtime': 1.3838, 'eval_samples_per_second': 91.051, 'eval_steps_per_second': 5.781, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train and test on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. HPO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TxM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
